{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192540b9-56f4-4e25-b961-99adc7b8cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec7d54e-9e23-4ae5-8c78-e9795ac32bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download the MNIST datasets\n",
    "folder = \"data\"\n",
    "trainset = datasets.MNIST(folder, train=True, download=True)\n",
    "testset = datasets.MNIST(folder, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05316283-68f1-4c1d-9d81-07ac4b7cfd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples 60000\n",
      "Total number of testing samples 10000\n",
      "5\n",
      "Max pixel value 1.0, min pixel value 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "print(f\"Total number of training samples {len(trainset)}\")\n",
    "print(f\"Total number of testing samples {len(testset)}\")\n",
    "img, label = trainset[0]\n",
    "plt.imshow(img)\n",
    "print(label)\n",
    "print(f\"Max pixel value {to_tensor(img).max()}, min pixel value {to_tensor(img).min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599d4aac-fed5-4e66-a4a2-2d20ef9c2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the MNIST dataset by converting to tensor and normalizing the pixel values\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST(folder, train=True, download=False, transform=to_tensor), \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(datasets.MNIST(folder, train=False, download=False, transform=to_tensor),\n",
    "                         batch_size=batch_size,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d9d2ec-1cbb-4b9a-b896-d36656aec6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # Basic implementation as MLP\n",
    "    def __init__(self, ninputs: int, nhidden: int, nlatent: int):\n",
    "        super().__init__()\n",
    "        self.ninputs = ninputs\n",
    "        self.nhidden = nhidden\n",
    "        nhidden2 = nhidden//2\n",
    "        self.f_z = nn.Sequential(nn.Linear(ninputs, nhidden),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(nhidden, nhidden2),\n",
    "                                 nn.ReLU())\n",
    "        self.f_mu = nn.Linear(nhidden2, nlatent)\n",
    "        self.f_sigma = nn.Linear(nhidden2, nlatent)\n",
    "        self.f_L = nn.Linear(nhidden2, nlatent * nlatent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z_         = self.f_z(x)\n",
    "        mu         = self.f_mu(z_)\n",
    "        log_sigma  = self.f_sigma(z_)\n",
    "        L_         = self.f_L(z_)\n",
    "        return mu, log_sigma, L_\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ninputs: int, nhidden: int, nlatent: int):\n",
    "        super().__init__()\n",
    "        self.ninputs = ninputs\n",
    "        self.nhidden = nhidden\n",
    "        nhidden2 = nhidden//2\n",
    "        self.f = nn.Sequential(nn.Linear(nlatent, nhidden2),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(nhidden2, nhidden),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(nhidden, ninputs),\n",
    "                               nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, z):\n",
    "        p = self.f(z)\n",
    "        return p\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, ninputs: int, nhidden: int, nlatent: int, nbatch: int, device):\n",
    "        super().__init__()\n",
    "        self.nbatch = nbatch\n",
    "        self.nlatent = nlatent\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(ninputs, nhidden, nlatent)\n",
    "        self.decoder = Decoder(ninputs, nhidden, nlatent)\n",
    "        \n",
    "    def loss(self, x):\n",
    "        mu, log_sigma, L_ = self.encoder(x)\n",
    "        L          = torch.tril(L_.view((-1, self.nlatent, self.nlatent)), -1) + torch.diag_embed(torch.exp(log_sigma))\n",
    "        L          = torch.diag_embed(torch.exp(log_sigma))\n",
    "        eps        = torch.normal(torch.zeros((self.nbatch, self.nlatent, 1)), torch.ones((self.nbatch, self.nlatent, 1))).to(self.device)\n",
    "        z_         = torch.squeeze(torch.bmm(L, eps))\n",
    "        z          = z_ + mu\n",
    "        ELBO_logqz = -1/2*(torch.sum(eps*eps, dim=1) + self.nlatent*np.log(2*torch.pi) + torch.sum(log_sigma, dim=1))\n",
    "        ELBO_logpz = -1/2*(torch.sum(z*z, dim=1) + self.nlatent*np.log(2*torch.pi))\n",
    "        p = self.decoder(z)\n",
    "        ELBO_logpx = torch.sum(x*torch.log(p)+(1-x)*torch.log(1-p), dim=1)\n",
    "        ELBO = ELBO_logpx + ELBO_logpz - ELBO_logqz\n",
    "        return -torch.mean(ELBO)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2606d5ac-3c61-41a2-9130-ae84f2026944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "img, label = trainset[0]\n",
    "ninputs = len(to_tensor(img).view(-1))\n",
    "print(ninputs)\n",
    "nhidden = 512\n",
    "nlatent = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0656e2a0-e5d6-494e-8c99-4a642b5bb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average ELBO estimate -234.86463928222656 (train), -190.19378662109375 (test).\n",
      "Epoch 1, average ELBO estimate -177.19129943847656 (train), -162.4114532470703 (test).\n",
      "Epoch 2, average ELBO estimate -152.77574157714844 (train), -146.55313110351562 (test).\n",
      "Epoch 3, average ELBO estimate -143.97503662109375 (train), -141.1051788330078 (test).\n",
      "Epoch 4, average ELBO estimate -139.5066375732422 (train), -137.495361328125 (test).\n",
      "Epoch 5, average ELBO estimate -136.38063049316406 (train), -134.77662658691406 (test).\n",
      "Epoch 6, average ELBO estimate -133.95184326171875 (train), -132.52781677246094 (test).\n",
      "Epoch 7, average ELBO estimate -131.93858337402344 (train), -130.75350952148438 (test).\n",
      "Epoch 8, average ELBO estimate -130.20132446289062 (train), -129.11708068847656 (test).\n",
      "Epoch 9, average ELBO estimate -128.73402404785156 (train), -127.84265899658203 (test).\n",
      "Epoch 10, average ELBO estimate -127.46198272705078 (train), -126.6576156616211 (test).\n",
      "Epoch 11, average ELBO estimate -126.38488006591797 (train), -125.63697052001953 (test).\n",
      "Epoch 12, average ELBO estimate -125.4813232421875 (train), -124.9155044555664 (test).\n",
      "Epoch 13, average ELBO estimate -124.65504455566406 (train), -124.40869140625 (test).\n",
      "Epoch 14, average ELBO estimate -123.90999603271484 (train), -123.52861785888672 (test).\n",
      "Epoch 15, average ELBO estimate -123.22505187988281 (train), -122.89763641357422 (test).\n",
      "Epoch 16, average ELBO estimate -122.62509155273438 (train), -122.44831848144531 (test).\n",
      "Epoch 17, average ELBO estimate -122.05937957763672 (train), -121.90994262695312 (test).\n",
      "Epoch 18, average ELBO estimate -121.52510833740234 (train), -121.35138702392578 (test).\n",
      "Epoch 19, average ELBO estimate -121.05635833740234 (train), -121.01995849609375 (test).\n",
      "Epoch 20, average ELBO estimate -120.59859466552734 (train), -120.54165649414062 (test).\n",
      "Epoch 21, average ELBO estimate -120.17948913574219 (train), -120.21273040771484 (test).\n",
      "Epoch 22, average ELBO estimate -119.78810119628906 (train), -119.75054931640625 (test).\n",
      "Epoch 23, average ELBO estimate -119.38864135742188 (train), -119.53841400146484 (test).\n",
      "Epoch 24, average ELBO estimate -119.03599548339844 (train), -119.22943878173828 (test).\n",
      "Epoch 25, average ELBO estimate -118.67595672607422 (train), -118.87065887451172 (test).\n",
      "Epoch 26, average ELBO estimate -118.35449981689453 (train), -118.62152862548828 (test).\n",
      "Epoch 27, average ELBO estimate -118.04950714111328 (train), -118.35161590576172 (test).\n",
      "Epoch 28, average ELBO estimate -117.76228332519531 (train), -118.10629272460938 (test).\n",
      "Epoch 29, average ELBO estimate -117.48497772216797 (train), -117.88351440429688 (test).\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "n_epochs = 30\n",
    "device = 'cpu'\n",
    "model = VAE(ninputs, nhidden, nlatent, batch_size, device).to(device)\n",
    "#nn.init.uniform_(model.encoder.f_L.weight, 0, 0) \n",
    "#nn.init.uniform_(model.encoder.f_L.bias, 0, 0)\n",
    "nn.init.uniform_(model.encoder.f_sigma.weight, 0, 0)\n",
    "nn.init.uniform_(model.encoder.f_sigma.bias, 0, 0)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_batch_train = len(train_loader)\n",
    "n_batch_test  = len(test_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        imgs = imgs.view(imgs.shape[0], -1)\n",
    "        loss = model.loss(imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for imgs, _ in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            imgs = imgs.view(imgs.shape[0], -1)\n",
    "            loss = model.loss(imgs)\n",
    "            test_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch}, average ELBO estimate {-train_loss/n_batch_train} (train), {-test_loss/n_batch_test} (test).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f9e631-bee2-4eac-8819-bb6676a1e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn((batch_size, nlatent))\n",
    "    sample = model.decoder(z)\n",
    "    \n",
    "save_image(sample.view(batch_size, 1, 28, 28), './MNIST_gen.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910262f-b665-43c6-a278-5fb904fcefd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pytorch_clean]",
   "language": "python",
   "name": "conda-env-miniconda3-pytorch_clean-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2555304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5151bb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-1.0019782 , -0.01461947,  0.6765263 ], dtype=float32),\n",
       " DeviceArray(1.7750063, dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(1234)\n",
    "mu = jnp.array([-1., 0., 1.])\n",
    "sigma = jnp.array([0.1, 0.2, 0.3])\n",
    "\n",
    "normal_dist = distrax.MultivariateNormalDiag(mu, sigma)\n",
    "samples, log_proba = normal_dist.sample_and_log_prob(seed=key)\n",
    "samples, log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910bd09",
   "metadata": {},
   "source": [
    "# Understanding some terminologies...\n",
    "\n",
    "This step is important since distrax is inspired by the TensorFlow Probability (tfp) library. We borrow these notes from tfp or distrax docs.\n",
    "\n",
    "## 1. distribution shapes\n",
    "\n",
    "There are three important concepts associated with TensorFlow Distributions shapes:\n",
    "\n",
    "* Event shape describes the shape of a single draw from the distribution; it may be dependent across dimensions. For scalar distributions, the event shape is `[]`. For a 5-dimensional MultivariateNormal, the event shape is `[5]`.\n",
    "* Batch shape describes independent, not identically distributed draws, aka a \"batch\" of distributions.\n",
    "* Sample shape describes independent, identically distributed draws of batches from the distribution family.\n",
    "\n",
    "The event shape and the batch shape are properties of a Distribution object, whereas the sample shape is associated with a specific call to sample or log_prob.\n",
    "\n",
    "Some experiments now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a2ba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_distributions = [\n",
    "    distrax.Uniform(low=0., high=1.),\n",
    "    distrax.Uniform(low=[0., 0., 0.], high=[1., 1., 1.]),\n",
    "    distrax.Uniform(low=jnp.zeros((2,3)), high=jnp.ones((2,3))),\n",
    "    distrax.Uniform(low=[0.], high=[1.]),\n",
    "    distrax.Uniform(low=[[0.]], high=[[1.]]),\n",
    "    distrax.Uniform(low=0., high=jnp.ones((2, 2)))\n",
    "] # event shape is [], batch shape decided by low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9e9aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.29453015, dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[0].sample(seed=key) # one uniform scalar batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ae7141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.49210894, 0.4708643 , 0.14046204], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[1].sample(seed=key) # three uniforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bddfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.8177053 , 0.17224324, 0.24385035],\n",
       "             [0.03261805, 0.6770656 , 0.8112081 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[2].sample(seed=key) # 2 by 3 uniforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83cc5cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.29453015], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[3].sample(seed=key) # vector batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac2d1e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.29453015]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[4].sample(seed=key) # expanded batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9498736e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.49210894, 0.44287562],\n",
       "             [0.14046204, 0.10368097]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_distributions[5].sample(seed=key) # parameter broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc8c02",
   "metadata": {},
   "source": [
    "The basic rule is that when we sample from a distribution, the resulting Tensor has shape `[sample_shape, batch_shape, event_shape]`, where batch_shape and event_shape are provided by the Distribution object, and sample_shape is provided by the call to sample. For scalar distributions, event_shape = `[]`, so the Tensor returned from sample will have shape `[sample_shape, batch_shape]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1a8f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_sample_tensor_shape(sample_shape, distribution):\n",
    "    print('Sample shape:', sample_shape)\n",
    "    print('Returned sample tensor shape:',\n",
    "          distribution.sample(sample_shape=sample_shape, seed=key).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d68c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (3, 2)\n",
      "Returned sample tensor shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "describe_sample_tensor_shape((3,2), uniform_distributions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd9ea4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<distrax._src.distributions.uniform.Uniform object at 0x13c271090>\n",
      "Sample shape: ()\n",
      "Returned sample tensor shape: (1,)\n",
      "Sample shape: 1\n",
      "Returned sample tensor shape: (1, 1)\n",
      "Sample shape: 2\n",
      "Returned sample tensor shape: (2, 1)\n",
      "Sample shape: [1, 5]\n",
      "Returned sample tensor shape: (1, 5, 1)\n",
      "Sample shape: [3, 4, 5]\n",
      "Returned sample tensor shape: (3, 4, 5, 1)\n",
      "<distrax._src.distributions.uniform.Uniform object at 0x13c159d20>\n",
      "Sample shape: ()\n",
      "Returned sample tensor shape: (3,)\n",
      "Sample shape: 1\n",
      "Returned sample tensor shape: (1, 3)\n",
      "Sample shape: 2\n",
      "Returned sample tensor shape: (2, 3)\n",
      "Sample shape: [1, 5]\n",
      "Returned sample tensor shape: (1, 5, 3)\n",
      "Sample shape: [3, 4, 5]\n",
      "Returned sample tensor shape: (3, 4, 5, 3)\n",
      "<distrax._src.distributions.uniform.Uniform object at 0x160367e50>\n",
      "Sample shape: ()\n",
      "Returned sample tensor shape: (2, 3)\n",
      "Sample shape: 1\n",
      "Returned sample tensor shape: (1, 2, 3)\n",
      "Sample shape: 2\n",
      "Returned sample tensor shape: (2, 2, 3)\n",
      "Sample shape: [1, 5]\n",
      "Returned sample tensor shape: (1, 5, 2, 3)\n",
      "Sample shape: [3, 4, 5]\n",
      "Returned sample tensor shape: (3, 4, 5, 2, 3)\n",
      "<distrax._src.distributions.uniform.Uniform object at 0x16031ec50>\n",
      "Sample shape: ()\n",
      "Returned sample tensor shape: (1,)\n",
      "Sample shape: 1\n",
      "Returned sample tensor shape: (1, 1)\n",
      "Sample shape: 2\n",
      "Returned sample tensor shape: (2, 1)\n",
      "Sample shape: [1, 5]\n",
      "Returned sample tensor shape: (1, 5, 1)\n",
      "Sample shape: [3, 4, 5]\n",
      "Returned sample tensor shape: (3, 4, 5, 1)\n",
      "<distrax._src.distributions.uniform.Uniform object at 0x12fb6a350>\n",
      "Sample shape: ()\n",
      "Returned sample tensor shape: (1, 1)\n",
      "Sample shape: 1\n",
      "Returned sample tensor shape: (1, 1, 1)\n",
      "Sample shape: 2\n",
      "Returned sample tensor shape: (2, 1, 1)\n",
      "Sample shape: [1, 5]\n",
      "Returned sample tensor shape: (1, 5, 1, 1)\n",
      "Sample shape: [3, 4, 5]\n",
      "Returned sample tensor shape: (3, 4, 5, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "def describe_sample_tensor_shapes(distributions, sample_shapes):\n",
    "    started = False\n",
    "    for distribution in distributions:\n",
    "      print(distribution)\n",
    "      for sample_shape in sample_shapes:\n",
    "        describe_sample_tensor_shape(sample_shape, distribution)\n",
    "\n",
    "sample_shapes = [(), 1, 2, [1, 5], [3, 4, 5]]\n",
    "describe_sample_tensor_shapes(poisson_distributions, sample_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60281",
   "metadata": {},
   "source": [
    "We note that in case where sample and/or event shape is (), the corresponding slot in the overall Tensor shape will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a26be0",
   "metadata": {},
   "source": [
    "Now let's take a look at log_prob, which is somewhat trickier. log_prob takes as input a (non-empty) tensor representing the location(s) at which to compute the log_prob for the distribution. In the most straightforward case, this tensor will have a shape of the form `[sample_shape, batch_shape, event_shape]`, where batch_shape and event_shape match the batch and event shapes of the distribution. Recall once more that for scalar distributions, event_shape = `[]`, so the input tensor has shape `[sample_shape, batch_shape]`. In this case, we get back a tensor of shape `[sample_shape, batch_shape]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f45edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, log_proba = uniform_distributions[0].sample_and_log_prob(sample_shape=(), seed=key)\n",
    "log_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "872a1f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, log_proba = uniform_distributions[0].sample_and_log_prob(sample_shape=(1), seed=key)\n",
    "log_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63810b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, log_proba = uniform_distributions[0].sample_and_log_prob(sample_shape=(1,2,3), seed=key)\n",
    "log_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c5f4b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, log_proba = uniform_distributions[1].sample_and_log_prob(sample_shape=(1), seed=key)\n",
    "log_proba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ef488",
   "metadata": {},
   "source": [
    "okay, that's straightforward enough! Now onto reading about the MaskedCoupling layer in the distrax library.\n",
    "\n",
    "The masking layer has forward operation of \n",
    "$$\n",
    "y = (1-m) \\cdot f(x; g(m\\cdot x)) + m\\cdot x,\n",
    "$$\n",
    "where $m$ is the binary mask array of the same dimension as $x$, $\\cdot$ is the elementwise multiplication op, and $g$ is the conditioner function that transforms on the masked input and produces output to be conditioned on in the inner bijector $f$. \n",
    "\n",
    "In the RealNVP model, the conditioner is typically a neural network. For simplicity we use an MLP here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "737c8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "Array = jnp.array\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "def make_conditioner(event_shape: Sequence[int],\n",
    "                     hidden_sizes: Sequence[int], \n",
    "                     num_bijector_params: int) -> hk.Sequential:\n",
    "    return hk.Sequential([\n",
    "        hk.Flatten(preserve_dims=-len(event_shape)), # so flatten all event dimensions\n",
    "        hk.nets.MLP(hidden_sizes, activate_final=True), # core MLP\n",
    "        # final projection, set weight=0 to start from the identity flow\n",
    "        hk.Linear(\n",
    "          np.prod(event_shape) * num_bijector_params,\n",
    "          w_init=jnp.zeros,\n",
    "          b_init=jnp.zeros),\n",
    "        hk.Reshape(tuple(event_shape) + (num_bijector_params,), preserve_dims=-1), # unflatten the last dim to event shape\n",
    "    ])\n",
    "\n",
    "# affine bijector as used in RealNVP\n",
    "def bijector_fn(params: Array):\n",
    "    shift, log_scale = params[..., 0], params[..., 1]\n",
    "    return distrax.ScalarAffine(shift=shift,\n",
    "                               log_scale=log_scale)\n",
    "\n",
    "def make_flow(num_layers = 5,\n",
    "              event_shape = [2],\n",
    "              hidden_sizes = [4, 4, event_shape[0]*2]) -> distrax.Transformed:\n",
    "\n",
    "    # Alternating binary mask.\n",
    "    mask = jnp.arange(0, np.prod(event_shape)) % 2\n",
    "    mask = jnp.reshape(mask, event_shape)\n",
    "    mask = mask.astype(bool)\n",
    "\n",
    "    layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layer = distrax.MaskedCoupling(\n",
    "            mask=mask,\n",
    "            bijector=bijector_fn,\n",
    "            conditioner=make_conditioner(event_shape, hidden_sizes, 2)\n",
    "        )\n",
    "        layers.append(layer)\n",
    "        mask = jnp.logical_not(mask) \n",
    "    \n",
    "        flow = distrax.Inverse(distrax.Chain(layers))\n",
    "        base_dist = distrax.MultivariateNormalDiag(loc=jnp.zeros(shape=event_shape),\n",
    "                                                   scale_diag=jnp.ones(shape=event_shape))\n",
    "        \n",
    "        return distrax.Transformed(base_dist, flow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd8abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax]",
   "language": "python",
   "name": "conda-env-jax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
